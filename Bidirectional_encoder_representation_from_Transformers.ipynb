{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manders2/NodeRed_AWS_Rekognition/blob/master/Bidirectional_encoder_representation_from_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSz5jzj61nHc"
      },
      "source": [
        "This code is possible because of [Tae-Hwan Jung](https://github.com/graykode). I have just broken down the code and added few things here and here for better understanding.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8kZmr4ItGUj"
      },
      "source": [
        "import math\n",
        "import re\n",
        "from random import *\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from pprint import pprint"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6YMNvc8tbA9"
      },
      "source": [
        "text = (\n",
        "        'Hello, how are you? I am Romeo.\\n'\n",
        "        'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
        "        'Nice meet you too. How are you today?\\n'\n",
        "        'Great. My baseball team won the competition.\\n'\n",
        "        'Oh Congratulations, Juliet\\n'\n",
        "        'Thanks you Romeo'\n",
        "    )"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhX8b1ydtrVf"
      },
      "source": [
        "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
        "word_list = list(set(\" \".join(sentences).split()))\n",
        "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "\n",
        "\n",
        "for i, w in enumerate(word_list):\n",
        "    word_dict[w] = i + 4\n",
        "number_dict = {i: w for i, w in enumerate(word_dict)}\n",
        "vocab_size = len(word_dict)\n",
        "\n",
        "token_list = list()\n",
        "for sentence in sentences:\n",
        "    arr = [word_dict[s] for s in sentence.split()]\n",
        "    token_list.append(arr)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ42SFLKtsv_",
        "outputId": "89ead134-08a7-4569-e4b3-2394b4132b70"
      },
      "source": [
        "token_list"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[13, 19, 9, 15, 18, 7, 22],\n",
              " [13, 22, 16, 8, 21, 26, 4, 11, 20, 15],\n",
              " [4, 20, 15, 23, 19, 9, 15, 17],\n",
              " [25, 16, 5, 6, 14, 27, 28],\n",
              " [10, 12, 26],\n",
              " [24, 15, 22]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q03SGkfIu_Kd"
      },
      "source": [
        "maxlen = 30 # maximum of length\n",
        "batch_size = 6\n",
        "max_pred = 5  # max tokens of prediction\n",
        "n_layers = 6 # number of Encoder of Encoder Layer\n",
        "n_heads = 12 # number of heads in Multi-Head Attention\n",
        "d_model = 768 # Embedding Size\n",
        "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\n",
        "n_segments = 2"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtyOOmRntu8w"
      },
      "source": [
        "\n",
        "def make_batch():\n",
        "    batch = []\n",
        "    positive = negative = 0\n",
        "    while positive != batch_size/2 or negative != batch_size/2:\n",
        "        tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences))\n",
        "        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
        "\n",
        "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
        "\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        #MASK LM\n",
        "        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 15 % of tokens in one sentence\n",
        "\n",
        "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
        "                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
        "        shuffle(cand_maked_pos)\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        for pos in cand_maked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "            if random() < 0.8:  # 80%\n",
        "                input_ids[pos] = word_dict['[MASK]'] # make mask\n",
        "            elif random() < 0.5:  # 10%\n",
        "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
        "                input_ids[pos] = word_dict[number_dict[index]] # replace\n",
        "\n",
        "        # Zero Paddings\n",
        "        n_pad = maxlen - len(input_ids)\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "\n",
        "    #     # Zero Padding (100% - 15%) tokens\n",
        "        if max_pred > n_pred:\n",
        "            n_pad = max_pred - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
        "            positive += 1\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
        "            negative += 1\n",
        "    return batch\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1PGksqBNuZM"
      },
      "source": [
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgJwW4OaiXE2"
      },
      "source": [
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7_HC-Y0jC3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6835ea5-598c-4fff-8adb-054f44aef71c"
      },
      "source": [
        "batch = make_batch()\n",
        "pprint(batch)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[1,\n",
            "   13,\n",
            "   22,\n",
            "   19,\n",
            "   8,\n",
            "   21,\n",
            "   26,\n",
            "   4,\n",
            "   11,\n",
            "   3,\n",
            "   15,\n",
            "   2,\n",
            "   25,\n",
            "   16,\n",
            "   5,\n",
            "   6,\n",
            "   3,\n",
            "   27,\n",
            "   28,\n",
            "   2,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0],\n",
            "  [0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0],\n",
            "  [14, 16, 20, 0, 0],\n",
            "  [16, 3, 9, 0, 0],\n",
            "  False],\n",
            " [[1,\n",
            "   10,\n",
            "   12,\n",
            "   26,\n",
            "   2,\n",
            "   13,\n",
            "   22,\n",
            "   16,\n",
            "   8,\n",
            "   21,\n",
            "   26,\n",
            "   3,\n",
            "   11,\n",
            "   3,\n",
            "   15,\n",
            "   2,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0],\n",
            "  [0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0],\n",
            "  [20, 4, 0, 0, 0],\n",
            "  [13, 11, 0, 0, 0],\n",
            "  False],\n",
            " [[1,\n",
            "   10,\n",
            "   12,\n",
            "   26,\n",
            "   2,\n",
            "   24,\n",
            "   15,\n",
            "   22,\n",
            "   2,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0],\n",
            "  [0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0],\n",
            "  [22, 0, 0, 0, 0],\n",
            "  [7, 0, 0, 0, 0],\n",
            "  True],\n",
            " [[1,\n",
            "   4,\n",
            "   20,\n",
            "   15,\n",
            "   23,\n",
            "   3,\n",
            "   9,\n",
            "   15,\n",
            "   17,\n",
            "   2,\n",
            "   24,\n",
            "   3,\n",
            "   22,\n",
            "   2,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0],\n",
            "  [0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0],\n",
            "  [19, 15, 0, 0, 0],\n",
            "  [5, 11, 0, 0, 0],\n",
            "  False],\n",
            " [[1,\n",
            "   13,\n",
            "   3,\n",
            "   7,\n",
            "   8,\n",
            "   21,\n",
            "   26,\n",
            "   4,\n",
            "   11,\n",
            "   20,\n",
            "   15,\n",
            "   2,\n",
            "   4,\n",
            "   20,\n",
            "   15,\n",
            "   23,\n",
            "   19,\n",
            "   9,\n",
            "   15,\n",
            "   17,\n",
            "   2,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0],\n",
            "  [0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0],\n",
            "  [4, 16, 22, 0, 0],\n",
            "  [12, 3, 2, 0, 0],\n",
            "  True],\n",
            " [[1,\n",
            "   4,\n",
            "   20,\n",
            "   15,\n",
            "   23,\n",
            "   19,\n",
            "   9,\n",
            "   15,\n",
            "   3,\n",
            "   2,\n",
            "   25,\n",
            "   16,\n",
            "   5,\n",
            "   6,\n",
            "   14,\n",
            "   3,\n",
            "   22,\n",
            "   2,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0],\n",
            "  [0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   1,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0,\n",
            "   0],\n",
            "  [28, 17, 27, 0, 0],\n",
            "  [16, 8, 15, 0, 0],\n",
            "  True]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM1-FdPJi6p3"
      },
      "source": [
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhM1DCU_iYCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a771f0a-5977-4d03-8a81-2609778d5350"
      },
      "source": [
        "get_attn_pad_mask(input_ids, input_ids)[0][0], input_ids[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]),\n",
              " tensor([ 1, 13, 22, 19,  8, 21, 26,  4, 11,  3, 15,  2, 25, 16,  5,  6,  3, 27,\n",
              "         28,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9lY7NZSWypE",
        "outputId": "1e3f3fa4-e7ce-416a-8748-1166418f6350"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1, 13, 22, 19,  8, 21, 26,  4, 11,  3, 15,  2, 25, 16,  5,  6,  3, 27,\n",
            "         28,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1, 10, 12, 26,  2, 13, 22, 16,  8, 21, 26,  3, 11,  3, 15,  2,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1, 10, 12, 26,  2, 24, 15, 22,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1,  4, 20, 15, 23,  3,  9, 15, 17,  2, 24,  3, 22,  2,  0,  0,  0,  0,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1, 13,  3,  7,  8, 21, 26,  4, 11, 20, 15,  2,  4, 20, 15, 23, 19,  9,\n",
            "         15, 17,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1,  4, 20, 15, 23, 19,  9, 15,  3,  2, 25, 16,  5,  6, 14,  3, 22,  2,\n",
            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnay0LTDjE4S"
      },
      "source": [
        "\"\"\"Defines the BERT Model Architecture, creates a dense vector (768 deep) for each of the 3 inputs (toek, position, segment). 'forward' then combines the vectors and normalses them \"\"\"\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
        "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
        "        #this is the 'Z' calculation:\n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.norm(embedding)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The code you've selected defines the Embedding class, which is a crucial part of the BERT model's input layer. This class is responsible for creating the initial embeddings for the input tokens, incorporating information about the token itself, its position in the sequence, and the segment it belongs to (e.g., Sentence A or Sentence B). Let's break it down:\n",
        "\n",
        "__init__(self): This is the constructor for the Embedding class. It initializes four main components:\n",
        "\n",
        "self.tok_embed = nn.Embedding(vocab_size, d_model): This creates an embedding layer for the tokens themselves. It maps each unique word/token in your vocabulary (vocab_size) to a dense vector of size d_model (768 in this case). This vector captures the semantic meaning of the word.\n",
        "self.pos_embed = nn.Embedding(maxlen, d_model): This creates a positional embedding layer. Since Transformers process words in parallel and don't inherently know the order of words, a positional embedding is added. It maps each possible position up to maxlen (30 in this case) to a d_model-sized vector, providing the model with sequence order information.\n",
        "self.seg_embed = nn.Embedding(n_segments, d_model): This creates a segment embedding layer. In tasks like Next Sentence Prediction (NSP), BERT processes two sentences simultaneously. This embedding differentiates between tokens belonging to the first segment (Sentence A, represented by 0) and the second segment (Sentence B, represented by 1), with n_segments being 2.\n",
        "self.norm = nn.LayerNorm(d_model): This applies Layer Normalization to the combined embeddings. Layer Normalization helps stabilize the training process and improves performance by normalizing the sum of the embeddings across the feature dimension.\n",
        "\n",
        "forward(self, x, seg): This method defines how the input x (token IDs) and seg (segment IDs) are processed to produce the final embedding. Here's what happens:\n",
        "\n",
        "seq_len = x.size(1): It gets the length of the input sequence.\n",
        "pos = torch.arange(seq_len, dtype=torch.long): It generates a tensor representing the positions [0, 1, 2, ..., seq_len-1].\n",
        "pos = pos.unsqueeze(0).expand_as(x): This reshapes the position tensor to match the shape of the input token IDs x, allowing for element-wise addition.\n",
        "embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg): This is the core step where the three types of embeddings (token, position, and segment) are retrieved and summed together. This sum represents a rich embedding for each token, containing its semantic meaning, its position, and its sentence context.\n",
        "return self.norm(embedding): Finally, the combined embedding is passed through the LayerNorm layer before being returned. This normalized embedding is then fed into the subsequent layers of the BERT model.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "yWNb0Tmk_-80",
        "outputId": "8adf8322-6b84-424f-8590-dd24da0a63c8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nThe code you've selected defines the Embedding class, which is a crucial part of the BERT model's input layer. This class is responsible for creating the initial embeddings for the input tokens, incorporating information about the token itself, its position in the sequence, and the segment it belongs to (e.g., Sentence A or Sentence B). Let's break it down:\\n\\n__init__(self): This is the constructor for the Embedding class. It initializes four main components:\\n\\nself.tok_embed = nn.Embedding(vocab_size, d_model): This creates an embedding layer for the tokens themselves. It maps each unique word/token in your vocabulary (vocab_size) to a dense vector of size d_model (768 in this case). This vector captures the semantic meaning of the word.\\nself.pos_embed = nn.Embedding(maxlen, d_model): This creates a positional embedding layer. Since Transformers process words in parallel and don't inherently know the order of words, a positional embedding is added. It maps each possible position up to maxlen (30 in this case) to a d_model-sized vector, providing the model with sequence order information.\\nself.seg_embed = nn.Embedding(n_segments, d_model): This creates a segment embedding layer. In tasks like Next Sentence Prediction (NSP), BERT processes two sentences simultaneously. This embedding differentiates between tokens belonging to the first segment (Sentence A, represented by 0) and the second segment (Sentence B, represented by 1), with n_segments being 2.\\nself.norm = nn.LayerNorm(d_model): This applies Layer Normalization to the combined embeddings. Layer Normalization helps stabilize the training process and improves performance by normalizing the sum of the embeddings across the feature dimension.\\n\\nforward(self, x, seg): This method defines how the input x (token IDs) and seg (segment IDs) are processed to produce the final embedding. Here's what happens:\\n\\nseq_len = x.size(1): It gets the length of the input sequence.\\npos = torch.arange(seq_len, dtype=torch.long): It generates a tensor representing the positions [0, 1, 2, ..., seq_len-1].\\npos = pos.unsqueeze(0).expand_as(x): This reshapes the position tensor to match the shape of the input token IDs x, allowing for element-wise addition.\\nembedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg): This is the core step where the three types of embeddings (token, position, and segment) are retrieved and summed together. This sum represents a rich embedding for each token, containing its semantic meaning, its position, and its sentence context.\\nreturn self.norm(embedding): Finally, the combined embedding is passed through the LayerNorm layer before being returned. This normalized embedding is then fed into the subsequent layers of the BERT model.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHjj-1wXjsdI"
      },
      "source": [
        "\"\"\"The creates the Z matrix (= the output of the self attention layer)  \"\"\"\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        #'Score' value in illustrated guide:\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one (= special characters?).\n",
        "        #'Softmax' value in illustrated guide:\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        #'Softmax * Value' value (not described as 'context' in illustrated guide):\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The ScaledDotProductAttention class is a fundamental building block of the Transformer architecture, responsible for calculating the attention weights and context vectors. Here's a breakdown of its operation:\n",
        "\n",
        "Purpose: It computes scaled dot-product attention, which determines how much 'attention' a model should pay to different parts of the input sequence when processing each element.\n",
        "\n",
        "forward(self, Q, K, V, attn_mask) method: This method takes four inputs:\n",
        "\n",
        "Q (Query): Represents the current element(s) for which we want to compute attention. Its shape is [batch_size, ..., len_q, d_k] (where len_q is the sequence length of the query, and d_k is the dimension of the keys).\n",
        "K (Key): Represents all elements in the sequence that the query might attend to. Its shape is [batch_size, ..., len_k, d_k].\n",
        "V (Value): Contains the information associated with each key. Its shape is [batch_size, ..., len_k, d_v].\n",
        "attn_mask: A boolean mask used to prevent attention to certain positions (e.g., padding tokens). Values of True in the mask indicate positions to be ignored.\n",
        "Steps involved in the forward pass:\n",
        "\n",
        "Calculate Raw Attention Scores:\n",
        "\n",
        "scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\n",
        "This is the core of dot-product attention. It computes the dot product between the Query (Q) and the Key (K.transpose(-1, -2)). The transpose operation aligns the K matrix for the dot product, effectively calculating the similarity between each query and all keys.\n",
        "The result is then divided by np.sqrt(d_k). This scaling factor prevents the dot products from growing too large, which can lead to vanishing or exploding gradients during training, especially with large d_k values. It helps stabilize the softmax function.\n",
        "The scores tensor will have a shape like [batch_size, ..., len_q, len_k], where each element (i, j) indicates the similarity between query i and key j.\n",
        "Apply Attention Masking:\n",
        "\n",
        "scores.masked_fill_(attn_mask, -1e9)\n",
        "The attn_mask is applied to the scores. For any position where attn_mask is True (meaning that position should be ignored), the corresponding score is set to a very large negative number (e.g., -1e9).\n",
        "This ensures that when the softmax function is applied next, these masked positions will have an output probability close to zero, effectively preventing the model from attending to them.\n",
        "Compute Attention Weights:\n",
        "\n",
        "attn = nn.Softmax(dim=-1)(scores)\n",
        "A Softmax function is applied along the last dimension (dim=-1) of the scores tensor. This normalizes the scores into a probability distribution, where each value attn[i, j] represents the weight or importance that query i assigns to key j (and its corresponding value).\n",
        "The sum of these weights for each query across all keys will be 1.\n",
        "Compute Context Vector:\n",
        "\n",
        "context = torch.matmul(attn, V)\n",
        "Finally, the attention weights (attn) are multiplied by the Value (V) matrix. This operation creates a weighted sum of the Value vectors, where the weights are the attention probabilities calculated in the previous step.\n",
        "The resulting context vector is an aggregated representation of the input sequence, selectively focusing on the most relevant information (weighted by attn) for each query.\n",
        "Output: The method returns two tensors:\n",
        "\n",
        "context: The weighted sum of the values, representing the attended-to information.\n",
        "attn: The attention weights, indicating how much each query focused on each key.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "fLQcYXSTK7mT",
        "outputId": "beeec3ac-f391-4771-cd82-dc43072db4e4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nThe ScaledDotProductAttention class is a fundamental building block of the Transformer architecture, responsible for calculating the attention weights and context vectors. Here's a breakdown of its operation:\\n\\nPurpose: It computes scaled dot-product attention, which determines how much 'attention' a model should pay to different parts of the input sequence when processing each element.\\n\\nforward(self, Q, K, V, attn_mask) method: This method takes four inputs:\\n\\nQ (Query): Represents the current element(s) for which we want to compute attention. Its shape is [batch_size, ..., len_q, d_k] (where len_q is the sequence length of the query, and d_k is the dimension of the keys).\\nK (Key): Represents all elements in the sequence that the query might attend to. Its shape is [batch_size, ..., len_k, d_k].\\nV (Value): Contains the information associated with each key. Its shape is [batch_size, ..., len_k, d_v].\\nattn_mask: A boolean mask used to prevent attention to certain positions (e.g., padding tokens). Values of True in the mask indicate positions to be ignored.\\nSteps involved in the forward pass:\\n\\nCalculate Raw Attention Scores:\\n\\nscores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\\nThis is the core of dot-product attention. It computes the dot product between the Query (Q) and the Key (K.transpose(-1, -2)). The transpose operation aligns the K matrix for the dot product, effectively calculating the similarity between each query and all keys.\\nThe result is then divided by np.sqrt(d_k). This scaling factor prevents the dot products from growing too large, which can lead to vanishing or exploding gradients during training, especially with large d_k values. It helps stabilize the softmax function.\\nThe scores tensor will have a shape like [batch_size, ..., len_q, len_k], where each element (i, j) indicates the similarity between query i and key j.\\nApply Attention Masking:\\n\\nscores.masked_fill_(attn_mask, -1e9)\\nThe attn_mask is applied to the scores. For any position where attn_mask is True (meaning that position should be ignored), the corresponding score is set to a very large negative number (e.g., -1e9).\\nThis ensures that when the softmax function is applied next, these masked positions will have an output probability close to zero, effectively preventing the model from attending to them.\\nCompute Attention Weights:\\n\\nattn = nn.Softmax(dim=-1)(scores)\\nA Softmax function is applied along the last dimension (dim=-1) of the scores tensor. This normalizes the scores into a probability distribution, where each value attn[i, j] represents the weight or importance that query i assigns to key j (and its corresponding value).\\nThe sum of these weights for each query across all keys will be 1.\\nCompute Context Vector:\\n\\ncontext = torch.matmul(attn, V)\\nFinally, the attention weights (attn) are multiplied by the Value (V) matrix. This operation creates a weighted sum of the Value vectors, where the weights are the attention probabilities calculated in the previous step.\\nThe resulting context vector is an aggregated representation of the input sequence, selectively focusing on the most relevant information (weighted by attn) for each query.\\nOutput: The method returns two tensors:\\n\\ncontext: The weighted sum of the values, representing the attended-to information.\\nattn: The attention weights, indicating how much each query focused on each key.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X2rbGNMzl7o",
        "outputId": "39b36c8b-57cf-4ca7-a3b9-f10bbd415e29",
        "collapsed": true
      },
      "source": [
        "#This line creates an instance of the Embedding class. This Embedding object is responsible for generating the initial input embeddings by combining token, positional, and segment embeddings:\n",
        "emb = Embedding()\n",
        "#input_ids: These are the numerical IDs representing the tokens (words) in your input sentences.\n",
        "#segment_ids: These IDs differentiate between the first sentence (e.g., Sentence A) and the second sentence (e.g., Sentence B) in a pair,\n",
        "embeds = emb(input_ids, segment_ids)\n",
        "print(\"embed shape = \")\n",
        "embeds.shape\n",
        "print(\"embeds =\")\n",
        "print(embeds)\n",
        "\n",
        "#The mask is used by the attention mechanism to ensure that these padding tokens do not influence the attention calculations, as they don't carry meaningful information.\n",
        "attenM = get_attn_pad_mask(input_ids, input_ids)\n",
        "print(f'attenM = {attenM}')\n",
        "\n",
        "SDPA= ScaledDotProductAttention()(embeds, embeds, embeds, attenM)\n",
        "#S: This represents the context vector. It's the aggregated output from the attention mechanism, where each token's representation has been updated by attending to other relevant tokens in the sequence.\n",
        "\n",
        "#A: This holds the attention weights themselves. These weights indicate the degree of importance or focus each token placed on every other token in the sequence during the attention calculation.\n",
        "\n",
        "S, A = SDPA\n",
        "\n",
        "#print('Masks',masks[0][0])\n",
        "print(type(S))\n",
        "print()\n",
        "print('Scores: ', S[0][0],'\\n\\nAttention M: ', A[0][0])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embed shape = \n",
            "embeds =\n",
            "tensor([[[ 0.4982,  1.0711,  1.3752,  ..., -0.3988,  1.9671, -0.4713],\n",
            "         [-0.2710,  0.3323,  0.2045,  ..., -0.0031, -0.2884, -1.4491],\n",
            "         [ 0.7692,  1.7149,  0.0906,  ...,  1.8900, -0.3077, -0.3169],\n",
            "         ...,\n",
            "         [ 0.3025,  0.8070,  0.0482,  ..., -0.2979,  1.6750,  1.9187],\n",
            "         [-0.4437,  0.5502, -0.0962,  ...,  0.2328,  0.7451,  1.1985],\n",
            "         [ 0.2421,  0.4638,  1.3170,  ...,  0.7390,  2.0177,  0.7565]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "<class 'torch.Tensor'>\n",
            "\n",
            "Scores:  tensor([ 0.4982,  1.0711,  1.3752, -0.4609,  0.3087, -1.2488, -0.4829, -0.5188,\n",
            "         0.2156, -0.7260,  1.3531, -0.3259, -1.1257, -0.5167, -2.5883, -1.0706,\n",
            "        -0.5671,  0.4844, -1.8429, -1.1296, -0.3728, -1.4016, -0.4801,  1.0780,\n",
            "        -0.1070,  0.3843, -2.0709, -1.3220,  0.9495,  0.1514,  0.6227, -1.5753,\n",
            "        -1.0232,  0.2689,  0.3051, -0.2843, -0.6396, -0.0500,  0.1686, -1.2812,\n",
            "         0.6559,  0.7426,  1.4239, -1.1901, -0.0697,  0.5401, -0.1561,  0.2133,\n",
            "        -2.2570, -0.4386,  0.8479,  1.1491,  1.4318, -1.4565, -1.1048,  0.6009,\n",
            "         0.2618,  0.4607,  0.7289,  0.0315,  2.1630,  1.7571, -1.3552,  1.5894,\n",
            "        -0.4758,  0.3215, -0.4964,  1.0387,  0.6363,  1.0041, -0.7487,  0.6178,\n",
            "         1.9854,  1.3543,  0.4486, -0.5075,  0.4889,  0.0533, -1.8071,  0.2929,\n",
            "         0.8520,  0.5952, -1.3751, -0.9927, -2.2675, -1.3855,  1.5357,  0.5474,\n",
            "        -0.3407, -0.1687,  2.4882, -0.8796,  1.2598, -1.1443,  0.3801, -1.5539,\n",
            "        -2.1240,  0.6129,  0.7249, -0.0498, -0.5943,  0.3656,  0.5911, -0.0349,\n",
            "        -1.6697, -1.2497,  1.3834, -1.6729,  0.3153,  0.9901,  0.0539, -1.0894,\n",
            "        -0.9332, -0.2248, -0.0078,  0.4312,  1.0497, -0.9422, -0.7278,  0.6885,\n",
            "        -0.1704,  1.2517,  0.0464, -0.4223, -0.3782,  0.5695, -0.8131,  0.4322,\n",
            "        -0.0728,  0.7653,  1.2751, -1.8737,  1.1136, -0.4401, -0.1442,  1.0308,\n",
            "         0.6913,  0.5339, -0.0535, -0.5826, -0.8887, -3.5356,  0.4641,  0.8299,\n",
            "        -0.1837,  0.4329,  0.1298,  0.6968, -0.9694,  0.8938,  0.4373,  0.3237,\n",
            "         1.1016,  0.1058,  0.7942,  0.4101,  0.4640,  0.4301, -1.1960, -0.8626,\n",
            "         0.4238,  0.0924,  0.5122,  0.2051,  0.7014,  0.3651,  0.2182, -0.9389,\n",
            "         0.5683, -0.5295,  0.1564, -1.6774,  1.3412, -0.3097,  0.2211, -1.6393,\n",
            "        -0.6135,  0.5493,  0.7223, -1.2232, -1.6622, -2.0649, -1.3237,  1.3997,\n",
            "         0.6881, -1.3128, -0.3809,  1.3027, -0.9267, -0.6328, -0.0537, -0.5405,\n",
            "        -0.1209, -1.2757,  0.1832,  0.5035, -1.2688, -0.5863, -1.0919,  0.3230,\n",
            "         0.7178,  0.9960,  0.1991,  1.0873, -1.0263,  1.7680,  0.5197, -0.6149,\n",
            "        -2.8843,  1.4227,  0.3856, -0.3016, -0.6815,  0.1585,  0.5500, -0.1719,\n",
            "        -1.3809,  2.1142,  0.0698,  0.2222, -0.0813,  0.9369, -3.4639, -0.3975,\n",
            "         0.6490,  0.7650, -0.6542, -0.2812, -1.0402, -0.2639, -0.3901,  0.8480,\n",
            "        -0.4516,  0.7572,  2.4445,  2.2893, -0.5205, -0.6494, -1.1398,  0.1316,\n",
            "        -0.3759, -0.1686, -0.8420, -0.2255, -0.8958,  0.0368,  0.4555, -0.5774,\n",
            "         0.6581,  0.6211,  2.4481,  2.6028, -0.5114,  0.7242,  0.5640, -0.5764,\n",
            "         0.8989, -0.0146,  1.0490,  0.2373,  0.6687, -0.0534,  1.1416,  0.4029,\n",
            "        -0.3377, -0.0098,  1.1487,  1.4754, -0.4938,  0.1477,  0.5106, -0.8808,\n",
            "        -0.7168,  1.3488, -1.2442, -0.7071, -1.4634, -0.5146,  0.1398,  0.7236,\n",
            "         0.8063,  1.8969,  0.1539, -0.0791, -0.6789, -0.1650, -1.3733,  0.9012,\n",
            "         0.5989, -0.4801,  2.8417,  1.3951, -0.2271, -0.1576, -0.3482,  0.4483,\n",
            "         0.4324, -2.0411,  0.8056, -0.2620, -0.1281,  1.8492, -0.5583, -0.2304,\n",
            "        -0.5981,  0.8128, -0.8909,  0.4528,  0.1075,  0.2094,  0.7098,  0.1780,\n",
            "        -0.2870, -0.3959, -1.1163, -1.1195, -0.3306,  1.2280, -0.0885, -0.8513,\n",
            "         1.4506, -1.6141,  1.6803,  0.9595,  0.5963, -0.2624,  3.0758, -0.0247,\n",
            "        -0.1842,  0.5549, -0.9528, -0.4746, -1.0898, -1.2331,  1.2808, -0.3329,\n",
            "        -1.0686, -0.9829, -0.0172,  0.1905,  0.8470, -1.5631, -1.2990, -1.7321,\n",
            "         0.7803,  1.6195, -0.1267, -0.2122, -2.3897, -0.4187,  1.7299, -0.4890,\n",
            "         0.2700, -0.1448, -0.2635, -1.0656,  0.6862, -0.9660,  2.2790, -0.1159,\n",
            "        -0.4971,  1.2756, -1.2706, -0.6376, -1.2285, -0.2025, -0.9243,  1.5352,\n",
            "        -0.7304, -0.7160, -1.7173,  2.1731, -1.0501, -1.7260, -1.6553,  0.4048,\n",
            "        -1.2688,  0.5187, -0.2399,  0.2461, -0.6863, -0.1425, -1.2700, -1.3266,\n",
            "         2.1121,  0.6040, -0.1060, -0.6520,  0.4392, -0.7240, -0.8506, -0.8399,\n",
            "         1.2777, -0.6697,  0.4671,  0.2725, -0.2448, -0.6900, -1.1775,  0.5602,\n",
            "        -0.0567, -1.5802,  0.5868, -0.2427, -0.4086, -0.1674,  1.5067, -1.6200,\n",
            "        -1.5954,  0.8926, -0.7571, -0.2237,  1.2800, -0.2783, -2.4181, -0.3652,\n",
            "        -0.9576,  2.4395,  0.5526,  2.1521, -0.2454, -0.2971, -1.3305, -1.7648,\n",
            "        -0.4076, -0.0195, -1.1427,  0.9802,  0.1730,  0.3778, -1.1076, -0.3353,\n",
            "        -1.5410,  0.2247,  0.6791, -0.6581, -0.1729, -1.1124,  0.7531,  1.1050,\n",
            "         1.3540, -0.2512,  1.5417,  1.1381,  0.4500, -0.0094,  0.7395, -0.0643,\n",
            "        -0.5837, -1.7437,  1.2084,  0.4923, -0.1793,  0.6611, -0.6973,  0.6116,\n",
            "        -0.7176, -0.5139, -0.0165,  1.5768, -0.3100, -0.1704,  2.1304, -0.6030,\n",
            "        -0.0746, -0.8956, -1.4670,  0.4087,  0.7053, -0.0854, -0.1082,  0.9216,\n",
            "         0.0509, -1.7563, -0.0308,  1.0264,  0.5110, -1.9313, -1.2590, -1.1501,\n",
            "         1.6945,  0.8991,  0.6324,  0.2346,  0.5670, -0.1885, -1.1305,  0.0345,\n",
            "        -1.5365,  0.4233,  1.9717,  0.3460, -0.7895,  0.0636,  0.1721,  2.2656,\n",
            "         0.9345,  0.3900, -0.5435,  1.4413,  0.1752,  0.4400, -0.1803,  0.2885,\n",
            "         0.4878, -0.5982,  1.0367, -0.6274, -0.6782,  1.1238,  0.0185, -1.9552,\n",
            "        -0.0132, -0.8176, -0.5409,  0.7241, -1.0245,  1.1695, -0.1260, -0.1176,\n",
            "        -0.3073,  0.8447, -0.8923,  0.3149, -0.7147, -0.3772, -0.4096, -0.2119,\n",
            "        -2.2187, -0.5504,  0.6893, -0.0867, -0.0709,  0.6146, -0.7364,  1.0932,\n",
            "        -0.6995,  1.4205,  1.5322,  0.6998,  0.2359, -0.6972, -0.5942,  1.6249,\n",
            "         0.2895, -1.3175,  1.0887, -0.3547,  0.3603,  0.0115,  1.2258,  0.9098,\n",
            "        -0.6610,  1.3763,  0.3713, -0.2138,  0.4980, -0.4052, -0.8467,  1.7240,\n",
            "        -0.7761,  1.8114,  0.5491, -0.6602, -0.0548,  0.1941,  0.0443, -0.7755,\n",
            "        -0.9705, -0.6270,  1.3179,  1.3505,  0.9829,  1.0082,  0.3481, -0.3603,\n",
            "         0.4727, -0.2367,  1.1023,  0.3970,  1.0886,  0.5372,  0.9657,  0.4897,\n",
            "         1.3336,  3.1635,  0.3580, -0.0488,  0.7130,  0.3815, -0.4855,  0.6658,\n",
            "        -0.9733,  0.7070,  0.5205, -0.0987, -0.4093,  0.7458,  1.8340, -2.3835,\n",
            "         2.5160,  0.2499,  0.1208, -0.1691, -0.9905,  1.6211,  0.5272,  0.9896,\n",
            "        -1.5188, -2.0522, -0.4932, -1.0483,  0.3119,  1.4226, -0.5867,  0.9643,\n",
            "         1.3322, -0.2458, -0.7998,  1.6313, -0.2673,  1.2457, -0.2586,  0.8294,\n",
            "        -0.6530, -0.4188, -1.6820,  0.9474, -1.6138, -1.2613,  2.3631,  0.9385,\n",
            "         0.1095,  0.3762, -0.6143, -0.2492,  0.7095,  0.8847, -0.9161, -0.7044,\n",
            "         0.2485,  0.9045, -1.7011, -0.3461,  0.3236, -1.0608,  0.6339, -0.1667,\n",
            "         0.5591, -1.2193,  0.1376, -0.5820, -1.7904,  0.3649, -0.1650,  0.4818,\n",
            "         1.2922, -1.0909, -0.2565, -1.2216,  0.6393, -0.1862, -0.6271, -1.2152,\n",
            "        -0.7269,  0.0266,  0.1636,  1.5711, -0.1875,  0.7913, -0.5423, -0.7006,\n",
            "         0.7746,  0.3916,  0.2667,  1.7557, -0.9503,  0.6024, -0.1494,  0.6204,\n",
            "         0.8348, -0.5030, -0.0138, -0.6018, -0.6485, -1.8157,  0.0791, -0.0402,\n",
            "         0.8421, -0.5973,  0.1155, -0.5199,  0.0723, -0.9411, -0.2678,  0.3378,\n",
            "        -0.8601,  1.4010, -0.9081,  1.1520,  0.2367,  0.1229, -0.2975,  0.9517,\n",
            "        -1.1538, -0.7375, -0.8958,  0.0714,  0.0959,  0.9127, -0.7828, -0.5783,\n",
            "         0.1541,  0.0696, -0.3893, -1.4805,  0.5590,  0.4713,  1.2941, -0.0489,\n",
            "        -0.9849,  0.5879,  0.2595,  0.5445, -0.3842,  1.1165, -0.2052,  0.3332,\n",
            "        -1.4779, -0.2364,  0.6737, -0.7763, -1.2319, -1.8420, -0.6361, -0.6670,\n",
            "        -0.0240,  1.3604, -1.1587, -1.8430,  0.3992, -0.0295, -0.5810,  1.1485,\n",
            "        -0.3942,  1.3403, -0.9212, -0.1335,  1.7385,  0.2958, -0.7048,  1.8114,\n",
            "         0.6710,  1.8495, -2.0273,  0.4304, -1.8816,  0.8504, -0.9594, -0.1561,\n",
            "         2.0857,  0.9071, -0.2746, -1.6669,  1.2910, -0.3988,  1.9671, -0.4713],\n",
            "       grad_fn=<SelectBackward0>) \n",
            "\n",
            "Attention M:  tensor([1.0000e+00, 1.5479e-29, 1.1099e-29, 4.0292e-29, 5.2837e-31, 2.4501e-27,\n",
            "        3.8816e-31, 1.3734e-29, 1.3341e-28, 1.0601e-28, 9.4286e-41, 6.4740e-43,\n",
            "        2.8306e-42, 8.1275e-44, 4.8765e-43, 1.1491e-43, 2.9848e-42, 3.3337e-42,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l4804HAuK1DD",
        "outputId": "7bcbf924-39c2-482b-d593-3928b05a8669"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((SDPA))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7Kcf93Y9eA3",
        "outputId": "001cb5d6-c246-4ca4-c5d6-67f9dcf4e517"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[[ 0.3216, -0.6705,  1.1729,  ...,  0.3254,  0.4016, -0.7950],\n",
            "         [-0.0717, -0.2142, -0.4410,  ..., -1.0927, -1.1948,  0.7350],\n",
            "         [ 0.5880,  0.5563,  0.1368,  ..., -0.1490, -0.1057,  1.8222],\n",
            "         ...,\n",
            "         [-0.2152, -1.1962,  0.5979,  ..., -0.9164,  0.7872,  1.1537],\n",
            "         [-0.1587, -1.1210,  0.4275,  ..., -0.7399,  0.5153,  1.0634],\n",
            "         [ 0.1424, -0.5979, -0.4271,  ..., -0.2504, -0.8059,  0.6594]],\n",
            "\n",
            "        [[ 0.3216, -0.6705,  1.1729,  ...,  0.3254,  0.4016, -0.7950],\n",
            "         [-0.4924,  0.0024, -0.3538,  ..., -0.5156, -0.9936,  0.3049],\n",
            "         [-0.3203, -0.1946,  2.0626,  ..., -1.0513,  0.3157,  1.0999],\n",
            "         ...,\n",
            "         [-0.4883, -0.0508, -0.2825,  ..., -0.3441, -1.0674,  0.4204],\n",
            "         [-0.4502, -0.2203, -0.0402,  ...,  0.1515, -1.2168,  0.6739],\n",
            "         [-0.3281, -0.1916,  0.1243,  ..., -0.0785, -0.8975,  0.6012]],\n",
            "\n",
            "        [[ 0.3216, -0.6705,  1.1729,  ...,  0.3254,  0.4016, -0.7950],\n",
            "         [-0.4924,  0.0024, -0.3538,  ..., -0.5156, -0.9936,  0.3049],\n",
            "         [-0.3203, -0.1946,  2.0626,  ..., -1.0513,  0.3157,  1.0999],\n",
            "         ...,\n",
            "         [-0.4883, -0.0508, -0.2825,  ..., -0.3441, -1.0674,  0.4204],\n",
            "         [-0.4502, -0.2203, -0.0402,  ...,  0.1515, -1.2168,  0.6739],\n",
            "         [-0.3281, -0.1916,  0.1243,  ..., -0.0785, -0.8975,  0.6012]],\n",
            "\n",
            "        [[ 0.3216, -0.6705,  1.1729,  ...,  0.3254,  0.4016, -0.7950],\n",
            "         [-0.1213, -1.5502, -0.1188,  ..., -0.2368, -1.0483,  0.9786],\n",
            "         [ 0.2779, -1.2908,  0.9511,  ..., -0.7195,  1.1538,  1.4838],\n",
            "         ...,\n",
            "         [-0.7861, -1.8979,  1.0148,  ..., -0.1213,  1.0400,  1.5541],\n",
            "         [-0.7972, -1.9510,  0.9410,  ..., -0.1455,  0.9145,  1.5388],\n",
            "         [-0.7094, -1.8658,  0.8467,  ..., -0.1355,  0.7481,  1.4900]],\n",
            "\n",
            "        [[ 0.3216, -0.6705,  1.1729,  ...,  0.3254,  0.4016, -0.7950],\n",
            "         [-0.0717, -0.2142, -0.4410,  ..., -1.0927, -1.1948,  0.7350],\n",
            "         [ 1.0448, -0.1222,  1.1553,  ..., -0.8052,  0.7910,  1.3535],\n",
            "         ...,\n",
            "         [-0.2408, -1.2047,  0.6144,  ..., -0.9638,  0.7963,  1.2018],\n",
            "         [-0.2674, -1.2152,  0.6631,  ..., -0.9570,  0.8384,  1.1936],\n",
            "         [-0.0561, -0.6224, -0.0979,  ..., -0.7653, -0.4294,  1.0588]],\n",
            "\n",
            "        [[ 0.3216, -0.6705,  1.1729,  ...,  0.3254,  0.4016, -0.7950],\n",
            "         [-0.1213, -1.5502, -0.1188,  ..., -0.2368, -1.0483,  0.9786],\n",
            "         [ 0.2779, -1.2908,  0.9511,  ..., -0.7195,  1.1538,  1.4838],\n",
            "         ...,\n",
            "         [-0.3697, -0.6425,  1.1464,  ..., -0.6348,  0.9003,  1.6026],\n",
            "         [-0.3585, -0.7465,  1.0227,  ..., -0.6129,  0.7141,  1.5492],\n",
            "         [-0.0709, -1.2375,  0.1228,  ..., -0.2008, -0.6226,  1.0901]]],\n",
            "       grad_fn=<UnsafeViewBackward0>), tensor([[[1.0000e+00, 9.8071e-27, 5.5133e-29,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [9.8068e-27, 1.0000e+00, 1.2097e-28,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [5.5133e-29, 1.2098e-28, 1.0000e+00,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         ...,\n",
            "         [7.7841e-06, 2.6641e-02, 5.0762e-04,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [1.4341e-03, 2.7538e-02, 3.5538e-04,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [3.1142e-04, 3.0331e-01, 8.7222e-04,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "        [[1.0000e+00, 1.5405e-27, 3.0921e-28,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [1.5405e-27, 1.0000e+00, 4.5014e-30,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [3.0920e-28, 4.5014e-30, 1.0000e+00,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         ...,\n",
            "         [2.5714e-04, 7.7401e-01, 4.1969e-04,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [4.9670e-02, 1.0934e-01, 2.5817e-03,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [2.4873e-03, 6.0838e-01, 3.0556e-03,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "        [[1.0000e+00, 1.5405e-27, 3.0921e-28,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [1.5405e-27, 1.0000e+00, 4.5014e-30,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [3.0920e-28, 4.5014e-30, 1.0000e+00,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         ...,\n",
            "         [2.5714e-04, 7.7401e-01, 4.1969e-04,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [4.9670e-02, 1.0934e-01, 2.5817e-03,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [2.4873e-03, 6.0838e-01, 3.0556e-03,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "        [[1.0000e+00, 8.1250e-29, 2.1925e-28,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [8.1247e-29, 1.0000e+00, 2.1995e-31,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [2.1925e-28, 2.1995e-31, 1.0000e+00,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         ...,\n",
            "         [4.1844e-05, 3.9338e-02, 1.8194e-05,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [8.5482e-04, 1.2333e-01, 8.6767e-07,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [1.3166e-04, 1.8072e-01, 2.9036e-06,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "        [[1.0000e+00, 9.8071e-27, 1.6736e-27,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [9.8068e-27, 1.0000e+00, 6.6326e-29,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [1.6736e-27, 6.6326e-29, 1.0000e+00,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         ...,\n",
            "         [7.7923e-06, 2.6669e-02, 3.8092e-05,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [1.6213e-03, 3.1133e-02, 6.5377e-05,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [3.6920e-04, 3.5959e-01, 3.0010e-05,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "        [[1.0000e+00, 8.1250e-29, 2.1925e-28,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [8.1247e-29, 1.0000e+00, 2.1995e-31,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [2.1925e-28, 2.1995e-31, 1.0000e+00,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         ...,\n",
            "         [2.3469e-05, 2.2064e-02, 1.0205e-05,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [8.8634e-04, 1.2788e-01, 8.9967e-07,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00],\n",
            "         [5.2535e-04, 7.2111e-01, 1.1586e-05,  ..., 0.0000e+00,\n",
            "          0.0000e+00, 0.0000e+00]]], grad_fn=<SoftmaxBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUX_eM_E1B8p"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
        "\n",
        "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
        "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The MultiHeadAttention class is a critical component in Transformer models, including BERT. It allows the model to jointly attend to information from different representation subspaces at different positions. Here's a detailed explanation:\n",
        "\n",
        "__init__(self): This is the constructor for the MultiHeadAttention class. It initializes three linear projection layers:\n",
        "\n",
        "self.W_Q = nn.Linear(d_model, d_k * n_heads): This layer projects the input query (Q) into n_heads separate 'query' vectors. The output dimension is d_k * n_heads, effectively preparing the input for multiple attention heads, each with a dimension of d_k.\n",
        "self.W_K = nn.Linear(d_model, d_k * n_heads): Similarly, this layer projects the input key (K) into n_heads separate 'key' vectors.\n",
        "self.W_V = nn.Linear(d_model, d_v * n_heads): This layer projects the input value (V) into n_heads separate 'value' vectors, each with a dimension of d_v.\n",
        "forward(self, Q, K, V, attn_mask): This method defines the forward pass of the multi-head attention mechanism:\n",
        "\n",
        "Store Residual Connection and Batch Size: residual, batch_size = Q, Q.size(0)\n",
        "\n",
        "It stores the original input Q for a residual connection later (a common practice in Transformers to help with training deep networks).\n",
        "It also extracts the batch_size from Q.\n",
        "Linear Projections and Reshaping (Splitting into Heads):\n",
        "\n",
        "q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
        "k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
        "v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)\n",
        "First, the Q, K, and V inputs (each of shape [batch_size, len_seq, d_model]) are passed through their respective linear projection layers (W_Q, W_K, W_V). The output now has a dimension of d_k * n_heads (or d_v * n_heads for V).\n",
        "Then, view is used to reshape these outputs to explicitly separate the n_heads. For example, (batch_size, len_seq, d_k * n_heads) becomes (batch_size, len_seq, n_heads, d_k).\n",
        "Finally, transpose(1,2) rearranges the dimensions to (batch_size, n_heads, len_seq, d_k) (or d_v for v_s). This arranges the tensors so that each 'head' can process its own Q, K, V independently.\n",
        "Prepare Attention Mask for Multi-Head: attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
        "\n",
        "The attn_mask (which is typically [batch_size, len_q, len_k]) is expanded to include the n_heads dimension, making it [batch_size, n_heads, len_q, len_k]. This ensures the same masking pattern is applied to all attention heads.\n",
        "Apply Scaled Dot-Product Attention: context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "\n",
        "The projected q_s, k_s, v_s, and the expanded attn_mask are passed to the ScaledDotProductAttention module (which you defined earlier). This performs the core attention calculation for each head independently, resulting in context vectors and attention weights attn for each head.\n",
        "Concatenate and Final Linear Projection: context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)\n",
        "\n",
        "output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "The context from all heads (currently [batch_size, n_heads, len_q, d_v]) are transposed back and then viewed (concatenated) to combine the outputs from all heads back into a single tensor of shape [batch_size, len_q, n_heads * d_v].\n",
        "This concatenated output then undergoes a final linear projection (nn.Linear(n_heads * d_v, d_model)) to bring the dimension back to d_model.\n",
        "Add Residual Connection and Layer Normalization: return nn.LayerNorm(d_model)(output + residual), attn\n",
        "\n",
        "The output from the linear projection is added to the original residual (input Q).\n",
        "Finally, Layer Normalization (nn.LayerNorm(d_model)) is applied to this sum.\n",
        "This entire process allows the model to capture diverse relationships within the sequence by attending to different parts of the input through multiple heads, and then consolidating these perspectives into a single, richer representation.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "bgznTLLzpReV",
        "outputId": "06dc627d-3f31-43a6-f5e3-4c1027f312aa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nThe MultiHeadAttention class is a critical component in Transformer models, including BERT. It allows the model to jointly attend to information from different representation subspaces at different positions. Here's a detailed explanation:\\n\\n__init__(self): This is the constructor for the MultiHeadAttention class. It initializes three linear projection layers:\\n\\nself.W_Q = nn.Linear(d_model, d_k * n_heads): This layer projects the input query (Q) into n_heads separate 'query' vectors. The output dimension is d_k * n_heads, effectively preparing the input for multiple attention heads, each with a dimension of d_k.\\nself.W_K = nn.Linear(d_model, d_k * n_heads): Similarly, this layer projects the input key (K) into n_heads separate 'key' vectors.\\nself.W_V = nn.Linear(d_model, d_v * n_heads): This layer projects the input value (V) into n_heads separate 'value' vectors, each with a dimension of d_v.\\nforward(self, Q, K, V, attn_mask): This method defines the forward pass of the multi-head attention mechanism:\\n\\nStore Residual Connection and Batch Size: residual, batch_size = Q, Q.size(0)\\n\\nIt stores the original input Q for a residual connection later (a common practice in Transformers to help with training deep networks).\\nIt also extracts the batch_size from Q.\\nLinear Projections and Reshaping (Splitting into Heads):\\n\\nq_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)\\nk_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)\\nv_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)\\nFirst, the Q, K, and V inputs (each of shape [batch_size, len_seq, d_model]) are passed through their respective linear projection layers (W_Q, W_K, W_V). The output now has a dimension of d_k * n_heads (or d_v * n_heads for V).\\nThen, view is used to reshape these outputs to explicitly separate the n_heads. For example, (batch_size, len_seq, d_k * n_heads) becomes (batch_size, len_seq, n_heads, d_k).\\nFinally, transpose(1,2) rearranges the dimensions to (batch_size, n_heads, len_seq, d_k) (or d_v for v_s). This arranges the tensors so that each 'head' can process its own Q, K, V independently.\\nPrepare Attention Mask for Multi-Head: attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\\n\\nThe attn_mask (which is typically [batch_size, len_q, len_k]) is expanded to include the n_heads dimension, making it [batch_size, n_heads, len_q, len_k]. This ensures the same masking pattern is applied to all attention heads.\\nApply Scaled Dot-Product Attention: context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\\n\\nThe projected q_s, k_s, v_s, and the expanded attn_mask are passed to the ScaledDotProductAttention module (which you defined earlier). This performs the core attention calculation for each head independently, resulting in context vectors and attention weights attn for each head.\\nConcatenate and Final Linear Projection: context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)\\n\\noutput = nn.Linear(n_heads * d_v, d_model)(context)\\nThe context from all heads (currently [batch_size, n_heads, len_q, d_v]) are transposed back and then viewed (concatenated) to combine the outputs from all heads back into a single tensor of shape [batch_size, len_q, n_heads * d_v].\\nThis concatenated output then undergoes a final linear projection (nn.Linear(n_heads * d_v, d_model)) to bring the dimension back to d_model.\\nAdd Residual Connection and Layer Normalization: return nn.LayerNorm(d_model)(output + residual), attn\\n\\nThe output from the linear projection is added to the original residual (input Q).\\nFinally, Layer Normalization (nn.LayerNorm(d_model)) is applied to this sum.\\nThis entire process allows the model to capture diverse relationships within the sequence by attending to different parts of the input through multiple heads, and then consolidating these perspectives into a single, richer representation.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs_xOAZy3pay",
        "outputId": "80c307c7-8f5b-4360-b8f8-cfcf0e4cd6a0"
      },
      "source": [
        "emb = Embedding()\n",
        "embeds = emb(input_ids, segment_ids)\n",
        "\n",
        "attenM = get_attn_pad_mask(input_ids, input_ids)\n",
        "\n",
        "MHA= MultiHeadAttention()(embeds, embeds, embeds, attenM)\n",
        "\n",
        "Output, A = MHA\n",
        "\n",
        "A[0][0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0811, 0.0463, 0.0512, 0.0525, 0.0412, 0.0477, 0.0698, 0.0349, 0.0417,\n",
              "         0.0967, 0.0334, 0.0533, 0.0511, 0.0266, 0.0381, 0.0241, 0.0753, 0.0638,\n",
              "         0.0390, 0.0321, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0647, 0.0319, 0.0508, 0.0575, 0.0405, 0.0570, 0.0457, 0.0469, 0.0379,\n",
              "         0.0583, 0.0359, 0.0455, 0.0578, 0.0461, 0.0557, 0.0390, 0.0697, 0.0644,\n",
              "         0.0463, 0.0486, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0951, 0.0372, 0.0379, 0.0567, 0.0397, 0.0408, 0.0563, 0.0312, 0.0721,\n",
              "         0.0775, 0.0468, 0.0355, 0.0520, 0.0331, 0.0422, 0.0428, 0.0845, 0.0493,\n",
              "         0.0423, 0.0270, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0676, 0.0289, 0.0375, 0.0566, 0.1116, 0.0365, 0.0441, 0.0207, 0.0470,\n",
              "         0.0742, 0.0566, 0.0444, 0.0458, 0.0384, 0.0345, 0.0226, 0.0898, 0.0619,\n",
              "         0.0333, 0.0482, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0474, 0.0354, 0.0589, 0.0584, 0.0517, 0.0560, 0.0455, 0.0514, 0.0512,\n",
              "         0.0512, 0.0631, 0.0479, 0.0410, 0.0392, 0.0260, 0.0393, 0.0614, 0.0847,\n",
              "         0.0424, 0.0479, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0616, 0.0337, 0.0520, 0.0486, 0.0543, 0.0527, 0.0470, 0.0482, 0.0421,\n",
              "         0.1014, 0.0450, 0.0415, 0.0437, 0.0282, 0.0449, 0.0308, 0.0847, 0.0646,\n",
              "         0.0396, 0.0354, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.1182, 0.0574, 0.0384, 0.0707, 0.0432, 0.0525, 0.0522, 0.0342, 0.0510,\n",
              "         0.0599, 0.0320, 0.0407, 0.0414, 0.0294, 0.0598, 0.0215, 0.0626, 0.0629,\n",
              "         0.0423, 0.0295, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0516, 0.0305, 0.0498, 0.0853, 0.0447, 0.0334, 0.0583, 0.0435, 0.0409,\n",
              "         0.0462, 0.0348, 0.0321, 0.0499, 0.0378, 0.0500, 0.0492, 0.0738, 0.0842,\n",
              "         0.0544, 0.0496, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0525, 0.0360, 0.0568, 0.0560, 0.0644, 0.0589, 0.0604, 0.0419, 0.0518,\n",
              "         0.0692, 0.0632, 0.0367, 0.0370, 0.0314, 0.0439, 0.0175, 0.0698, 0.0563,\n",
              "         0.0553, 0.0411, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0495, 0.0346, 0.0303, 0.0470, 0.0540, 0.0427, 0.0377, 0.0327, 0.0349,\n",
              "         0.0597, 0.0339, 0.0200, 0.0599, 0.0857, 0.0617, 0.0399, 0.0915, 0.0730,\n",
              "         0.0515, 0.0598, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0587, 0.0388, 0.0411, 0.0519, 0.0668, 0.0332, 0.0549, 0.0395, 0.0356,\n",
              "         0.0773, 0.0432, 0.0342, 0.0430, 0.0297, 0.0636, 0.0210, 0.0882, 0.0924,\n",
              "         0.0467, 0.0401, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0681, 0.0377, 0.0431, 0.0407, 0.0421, 0.0459, 0.0606, 0.0460, 0.0550,\n",
              "         0.0731, 0.0503, 0.0384, 0.0508, 0.0353, 0.0342, 0.0412, 0.0799, 0.0752,\n",
              "         0.0523, 0.0301, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0641, 0.0260, 0.0658, 0.0552, 0.0458, 0.0350, 0.0407, 0.0631, 0.0443,\n",
              "         0.0350, 0.0416, 0.0332, 0.0707, 0.0656, 0.0337, 0.0741, 0.0431, 0.0509,\n",
              "         0.0467, 0.0655, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0609, 0.0315, 0.0694, 0.0681, 0.0603, 0.0542, 0.0448, 0.0372, 0.0358,\n",
              "         0.0407, 0.0560, 0.0585, 0.0422, 0.0466, 0.0440, 0.0546, 0.0375, 0.0426,\n",
              "         0.0487, 0.0661, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0735, 0.0332, 0.0392, 0.0364, 0.0286, 0.0319, 0.0407, 0.0314, 0.0390,\n",
              "         0.0466, 0.0383, 0.0339, 0.0628, 0.0700, 0.0550, 0.0556, 0.0640, 0.0679,\n",
              "         0.0438, 0.1083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0490, 0.0421, 0.0335, 0.0507, 0.0456, 0.0370, 0.0290, 0.0502, 0.0309,\n",
              "         0.0535, 0.0287, 0.0414, 0.0774, 0.0608, 0.0613, 0.0997, 0.0604, 0.0474,\n",
              "         0.0499, 0.0514, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0605, 0.0338, 0.0617, 0.0499, 0.0464, 0.0334, 0.0380, 0.0398, 0.0303,\n",
              "         0.0366, 0.0378, 0.0396, 0.0492, 0.0886, 0.0417, 0.0497, 0.0581, 0.0759,\n",
              "         0.0408, 0.0881, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0360, 0.0479, 0.0772, 0.1118, 0.0274, 0.0386, 0.0706, 0.0606, 0.0652,\n",
              "         0.0414, 0.0435, 0.0646, 0.0361, 0.0409, 0.0437, 0.0476, 0.0331, 0.0232,\n",
              "         0.0419, 0.0487, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0435, 0.0282, 0.0528, 0.0663, 0.0488, 0.0319, 0.0531, 0.0464, 0.0583,\n",
              "         0.0476, 0.0358, 0.0465, 0.0672, 0.0611, 0.0376, 0.0579, 0.0543, 0.0466,\n",
              "         0.0494, 0.0667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0604, 0.0349, 0.0527, 0.0531, 0.0425, 0.0357, 0.0343, 0.0344, 0.0381,\n",
              "         0.0574, 0.0513, 0.0555, 0.0600, 0.0496, 0.0375, 0.0739, 0.0576, 0.0535,\n",
              "         0.0574, 0.0601, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0570, 0.0458, 0.0676, 0.0933, 0.0611, 0.0507, 0.0589, 0.0478, 0.0489,\n",
              "         0.0468, 0.0413, 0.0492, 0.0471, 0.0324, 0.0294, 0.0340, 0.0463, 0.0576,\n",
              "         0.0454, 0.0395, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0576, 0.0362, 0.0373, 0.0493, 0.0521, 0.0436, 0.0465, 0.0412, 0.0417,\n",
              "         0.0677, 0.0356, 0.0470, 0.0758, 0.0595, 0.0393, 0.0475, 0.0709, 0.0547,\n",
              "         0.0508, 0.0455, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0623, 0.0383, 0.0550, 0.0485, 0.0537, 0.0416, 0.0533, 0.0375, 0.0455,\n",
              "         0.0440, 0.0460, 0.0535, 0.0541, 0.0523, 0.0390, 0.0357, 0.0547, 0.0619,\n",
              "         0.0569, 0.0662, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0490, 0.0413, 0.0489, 0.0514, 0.0581, 0.0506, 0.0566, 0.0340, 0.0523,\n",
              "         0.0463, 0.0644, 0.0680, 0.0496, 0.0412, 0.0316, 0.0347, 0.0519, 0.0631,\n",
              "         0.0474, 0.0598, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0584, 0.0477, 0.0594, 0.0476, 0.0739, 0.0466, 0.0580, 0.0291, 0.0642,\n",
              "         0.0433, 0.0448, 0.0678, 0.0492, 0.0576, 0.0293, 0.0317, 0.0412, 0.0485,\n",
              "         0.0429, 0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0616, 0.0448, 0.0490, 0.0689, 0.0589, 0.0584, 0.0494, 0.0415, 0.0510,\n",
              "         0.0488, 0.0467, 0.0522, 0.0585, 0.0551, 0.0207, 0.0269, 0.0463, 0.0555,\n",
              "         0.0567, 0.0491, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0569, 0.0342, 0.0598, 0.0606, 0.0442, 0.0433, 0.0362, 0.0319, 0.0635,\n",
              "         0.0480, 0.0477, 0.0542, 0.0537, 0.0640, 0.0288, 0.0321, 0.0686, 0.0522,\n",
              "         0.0602, 0.0600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0666, 0.0605, 0.0515, 0.0660, 0.0544, 0.0531, 0.0768, 0.0395, 0.0531,\n",
              "         0.0487, 0.0514, 0.0528, 0.0515, 0.0359, 0.0301, 0.0307, 0.0396, 0.0495,\n",
              "         0.0434, 0.0448, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0473, 0.0610, 0.0685, 0.0580, 0.0401, 0.0567, 0.0677, 0.0382, 0.0540,\n",
              "         0.0628, 0.0421, 0.0490, 0.0546, 0.0605, 0.0266, 0.0314, 0.0575, 0.0476,\n",
              "         0.0368, 0.0397, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000],\n",
              "        [0.0511, 0.0552, 0.0526, 0.0416, 0.0690, 0.0378, 0.0604, 0.0313, 0.0415,\n",
              "         0.0609, 0.0438, 0.0442, 0.0505, 0.0526, 0.0257, 0.0361, 0.0589, 0.0808,\n",
              "         0.0595, 0.0468, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GQFL_Va4N4Y"
      },
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
        "        return self.fc2(gelu(self.fc1(x)))\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgmfjTqw4Qnw"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
        "        return enc_outputs, attn"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ0TJ84W4SZw"
      },
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embedding()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.activ1 = nn.Tanh()\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.activ2 = gelu\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        # decoder is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        n_vocab, n_dim = embed_weight.size()\n",
        "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
        "        self.decoder.weight = embed_weight\n",
        "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n",
        "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n",
        "\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
        "        # get masked position from final output of transformer.\n",
        "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
        "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
        "\n",
        "        return logits_lm, logits_clsf"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UAG3SEP4UbU",
        "outputId": "1e9a9442-2a30-4f07-ad97-9b44fa88b5e0"
      },
      "source": [
        "model = BERT()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "batch = make_batch()\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
        "\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
        "    loss_lm = (loss_lm.float()).mean()\n",
        "    loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
        "    loss = loss_lm + loss_clsf\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0010 cost = 34.293819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD3K8T6B4YJp",
        "outputId": "325cfe8b-edc0-4abc-f904-9e562c32eef8"
      },
      "source": [
        "# Predict mask tokens ans isNext\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
        "print(text)\n",
        "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
        "\n",
        "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
        "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
        "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
        "\n",
        "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext : ',True if logits_clsf else False)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, how are you? I am Romeo.\n",
            "Hello, Romeo My name is Juliet. Nice to meet you.\n",
            "Nice meet you too. How are you today?\n",
            "Great. My baseball team won the competition.\n",
            "Oh Congratulations, Juliet\n",
            "Thanks you Romeo\n",
            "['[CLS]', 'nice', 'meet', 'you', 'too', 'how', '[MASK]', '[MASK]', 'today', '[SEP]', 'great', 'my', 'baseball', 'team', 'won', 'the', 'competition', '[SEP]']\n",
            "masked tokens list :  [9, 6, 15]\n",
            "predict masked tokens list :  []\n",
            "isNext :  True\n",
            "predict isNext :  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf97uJJS4grJ"
      },
      "source": [],
      "execution_count": 29,
      "outputs": []
    }
  ]
}